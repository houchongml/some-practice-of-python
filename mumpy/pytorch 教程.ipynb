{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.empty(5,3)\n",
    "x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.empty()和torch.zeros()的主要区别在于它们初始化张量元素的方式，torch.empty()未初始化张量，torch.zeros()将张量的元素初始化为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8142, 0.0103, 0.1324],\n",
       "        [0.2860, 0.3174, 0.9101],\n",
       "        [0.0358, 0.9176, 0.5675],\n",
       "        [0.0217, 0.4247, 0.4713],\n",
       "        [0.0297, 0.5149, 0.0412]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[-0.6338, -1.6565, -0.9811],\n",
      "        [-1.3722, -0.1331,  1.2598],\n",
      "        [-0.7445,  0.2115,  0.0526],\n",
      "        [-0.9367,  0.2224,  1.3718],\n",
      "        [ 1.7385, -0.1596,  0.7639]])\n"
     ]
    }
   ],
   "source": [
    "#用new_方法创建对象\n",
    "x = x.new_ones(5,3,dtype=torch.double )#64 双精度\n",
    "print(x)\n",
    "x = torch.randn_like (x,dtype=torch.float)# 32 双精度\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加法运算 \n",
    "\n",
    "\n",
    "加法1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2243, -1.5971, -0.6673],\n",
      "        [-0.8673,  0.2088,  2.2556],\n",
      "        [-0.5025,  0.8107,  0.9395],\n",
      "        [-0.1228,  0.4929,  1.5021],\n",
      "        [ 1.9190,  0.1930,  1.3798]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5,3)\n",
    "print(x+y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加法2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2243, -1.5971, -0.6673],\n",
      "        [-0.8673,  0.2088,  2.2556],\n",
      "        [-0.5025,  0.8107,  0.9395],\n",
      "        [-0.1228,  0.4929,  1.5021],\n",
      "        [ 1.9190,  0.1930,  1.3798]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(x,y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 结果存储在 预设的tensor 中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2243, -1.5971, -0.6673],\n",
      "        [-0.8673,  0.2088,  2.2556],\n",
      "        [-0.5025,  0.8107,  0.9395],\n",
      "        [-0.1228,  0.4929,  1.5021],\n",
      "        [ 1.9190,  0.1930,  1.3798]])\n"
     ]
    }
   ],
   "source": [
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out= result)# 将结果存储在 out= 后面的变量中\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2243, -1.5971, -0.6673],\n",
      "        [-0.8673,  0.2088,  2.2556],\n",
      "        [-0.5025,  0.8107,  0.9395],\n",
      "        [-0.1228,  0.4929,  1.5021],\n",
      "        [ 1.9190,  0.1930,  1.3798]])\n"
     ]
    }
   ],
   "source": [
    "y.add_(x) # 把x 加上 y ，并且把值放在y里面\n",
    "print(y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE : 任何以'_' 结尾的操作，都会用结果替换原变量，例如``x.copy_(y)``, ``x.t_()``, 都会改变 ``x``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6338, -1.6565, -0.9811],\n",
      "        [-1.3722, -0.1331,  1.2598],\n",
      "        [-0.7445,  0.2115,  0.0526],\n",
      "        [-0.9367,  0.2224,  1.3718],\n",
      "        [ 1.7385, -0.1596,  0.7639]])\n",
      "tensor([-1.6565, -0.1331,  0.2115,  0.2224, -0.1596])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x[:,1]) # 第二列"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.view 可以改变张量的维度和大小  和 Numpy d reshape 类似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4,4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1,8) #确定列数，让 系统自己 确定 行数\n",
    "print(x.size(),y.size(),z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000])\n",
      "1.0000001192092896\n"
     ]
    }
   ],
   "source": [
    "#当 只有一个元素的 张量的 使用.item()来得到Python数据类型的数值\n",
    "x = torch.tensor([1.0000001])\n",
    "print(x)\n",
    "print(x.item())# 注意⚠️！！！  元素为小数的时候不能准确显示，会有较大的误差"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy 转换\n",
    "一个tensor 可以转换为 Numpy 反之亦然\n",
    "\n",
    "\n",
    "tensor 和 numpy 共享底层内存地址，修改一个会导致另外一个的变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# 将tensor 转换为 numpy \n",
    "a = torch.ones(5)\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.]) b [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# 观察numpy数组的值是如何改变的 \n",
    "a.add_(1)\n",
    "print(a,\"b\",b)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy array 转换为 torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [2. 2. 2. 2. 2.]\n",
      "b tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 使用from_numpy自动转化 \n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a) # 改变其中一个，两者都改变\n",
    "print('a',a)\n",
    "print('b',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available() and torch.cuda.is_macos_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "print(torch.backends.mps.is_available()) \n",
    "print(torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3.],\n",
       "        [3., 3.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x+2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x7fc7c938ff40>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) out tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, 'out',out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".requires_grad_(...)是PyTorch中张量（Tensor）的一个方法，用于动态地更改张量的requires_grad属性。\n",
    "\n",
    "当一个张量被创建时，可以使用requires_grad参数来指定是否需要计算该张量的梯度。但是，有时候需要动态地更改张量的requires_grad属性，例如在模型的训练过程中需要冻结部分参数。在这种情况下，就可以使用.requires_grad_(...)方法。\n",
    "\n",
    "\n",
    "False 就是冻结梯度运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x7fc7c93972b0>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,2)\n",
    "a = (a * 3) / (a - 1)\n",
    "print(a.requires_grad)# 默认是false\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(27., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度\n",
    "\n",
    "可以通过调用y.backward()来计算y相对于所有需要梯度的张量的梯度。\n",
    "\n",
    "如果需要将梯度存储到一个已经创建好的张量中，可以在backward()方法中指定一个out参数，该参数表示梯度将要被存储的张量 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = torch.zeros(2, 2)\n",
    "z.backward(out)\n",
    "out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.autograd模块提供了自动求导的功能，可以自动计算张量的梯度。自动求导是通过记录张量的操作历史来实现的。每个张量都有一个requires_grad属性，用于指示是否需要计算该张量的梯度。如果一个张量的requires_grad属性为True，那么它将被认为是一个需要求导的节点，并且所有的操作都将被记录下来。\n",
    "\n",
    "一旦所有的操作都完成了，可以调用backward()方法来计算所有需要梯度的张量的梯度。backward()方法会沿着操作历史反向传播梯度，并将梯度累加到每个张量的grad属性中。需要注意的是，对于同一个张量进行多次反向传播时，每次反向传播都会累加梯度到该张量的grad属性中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -32.8415, -967.9934,  395.6252], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x * 2 \n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "    pass\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个情形中，y不再是个标量。torch.autograd无法直接计算出完整的雅可比行列，但是如果我们只想要vector-Jacobian product，只需将向量作为参数传入backward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的是，只有在标量上调用backward()函数才是有效的。如果y不是标量，那么可以使用gradient参数来指定一个张量，该张量与y具有相同的形状，用于计算y的梯度。在这种情况下，gradient张量中的每个元素都将乘以相应的y的梯度，然后求和，以得到一个标量，最后将该标量用于反向传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(gradients)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "# 获取排除梯度的张量数据\n",
    "y_data = y.detach()\n",
    "print(y_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(91., grad_fn=<SumBackward0>)\n",
      "tensor([[ 2.,  4.,  6.],\n",
      "        [ 8., 10., 12.]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 创建一个张量\n",
    "x = torch.tensor([[1., 2., 3.], [4., 5., 6.]], requires_grad=True)\n",
    "\n",
    "# 进行一些操作\n",
    "y = x.pow(2).sum()\n",
    "print(y)\n",
    "\n",
    "# 计算梯度\n",
    "y.backward()\n",
    "\n",
    "# 输出梯度\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  4.,  9.],\n",
      "        [16., 25., 36.]], grad_fn=<PowBackward0>)\n",
      "tensor([[ 4., 12., 12.],\n",
      "        [ 8., 10., 12.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1., 2., 3.], [4., 5., 6.]], requires_grad=True)\n",
    "\n",
    "# 进行一些操作\n",
    "y = x.pow(2)\n",
    "print(y)\n",
    "\n",
    "# 计算梯度\n",
    "gradients = torch.tensor([[2,3,2],\n",
    "                          [1,1,1]])\n",
    "y.backward(gradients)\n",
    "\n",
    "# 输出梯度\n",
    "print(x.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------\n",
    "\n",
    "回顾autograd机制\n",
    "\n",
    "\n",
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2552, -1.7295, -0.5089, -0.1170],\n",
       "        [-2.4160,  1.3484, -0.2134,  1.9794],\n",
       "        [ 0.1945, -0.9690,  0.1436,  1.3074]], requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3,4,requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
